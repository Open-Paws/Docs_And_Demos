# Documentation, Demonstrations & Datasets

## Overview
This repository contains documentation and examples for building AI systems to support animal advocacy. Our ecosystem includes prediction models, a comprehensive knowledge graph and vector database, specialized models, and extensive datasets.

## Documentation

### üóÑÔ∏è Knowledge Infrastructure
Documentation for our vector-graph database:

   - [Overview & Quick Start](Knowledge/README.md)
   - Searching The Database
   - Understanding Our Data Structure

### üìä Prediction Models
Documentation for our performance and preference prediction models:

- [Overview & Quick Start](Predictions/README.md)

### ü§ñ Specialized Generative AI Models
Documentation for our specialized generative AI models, including Large Language Models (LLMs) and Visual Language Models (VLMs):

1. **3B Agentic Chat LoRA**
   - Overview & Quick Start
   - Best Practises & Usage Guidelines

2. **8B Reasoning LoRA**
   - Overview & Quick Start
   - Best Practises & Usage Guidelines

3. **11B Visual Language LoRA**
   - Overview & Quick Start
   - Best Practises & Usage Guidelines

## Demonstrations

Coming soon...

## Datasets
Access our HuggingFace datasets:

1. **Conversational Datasets**
   - Agentic conversations ([small](https://huggingface.co/datasets/open-paws/agentic_conversations_small)/[medium](https://huggingface.co/datasets/open-paws/agentic_conversations_medium)/[large](https://huggingface.co/datasets/open-paws/agentic_conversations_large))
   - Visual QA ([small](https://huggingface.co/datasets/open-paws/visual_qa_small)/[medium](https://huggingface.co/datasets/open-paws/visual_qa_medium)/[large](https://huggingface.co/datasets/open-paws/visual_qa_large))
   - Reasoning conversations ([small](https://huggingface.co/datasets/open-paws/reasoning_conversations_small) only)

2. **Core Datasets**
   - [Animal advocacy facts](https://huggingface.co/datasets/open-paws/animal_advocacy_facts)
   - [Animal alignment feedback](https://huggingface.co/datasets/open-paws/animal_alignment_feedback)
